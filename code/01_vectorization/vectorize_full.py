import json
import numpy as np
import pandas as pd
import torch
import time
from tqdm import tqdm
import re
import os
import sys

print("=== çº¯æœ¬åœ°å‘é‡åŒ–ï¼ˆå®Œå…¨ç¦»çº¿ï¼‰ ===")

# å½»åº•ç¦ç”¨ç½‘ç»œ
os.environ['TRANSFORMERS_OFFLINE'] = '1'
os.environ['HF_DATASETS_OFFLINE'] = '1' 
os.environ['HF_EVALUATE_OFFLINE'] = '1'
os.environ['TRANSFORMERS_VERBOSITY'] = 'error'

def clean_html(text):
    """æ¸…ç†HTMLæ ‡ç­¾"""
    if not text:
        return ""
    text = re.sub(r'<[^>]+>', ' ', text)
    text = re.sub(r'```.*?```', ' ', text, flags=re.DOTALL)
    text = re.sub(r'`[^`]*`', ' ', text)
    text = re.sub(r'\s+', ' ', text)
    return text.strip()

def combine_all_content(question_item):
    """ç»„åˆé—®é¢˜çš„æ‰€æœ‰æ–‡æœ¬å†…å®¹"""
    parts = []
    
    title = clean_html(question_item.get('title', ''))
    if title:
        parts.append(f"æ ‡é¢˜: {title}")
    
    body = clean_html(question_item.get('body', ''))
    if body:
        parts.append(f"é—®é¢˜: {body}")
    
    for i, comment in enumerate(question_item.get('comments', [])):
        comment_body = clean_html(comment.get('body', ''))
        if comment_body:
            parts.append(f"é—®é¢˜è¯„è®º{i+1}: {comment_body}")
    
    for j, answer in enumerate(question_item.get('answers', [])):
        answer_body = clean_html(answer.get('body', ''))
        if answer_body:
            parts.append(f"å›ç­”{j+1}: {answer_body}")
            
        for k, ans_comment in enumerate(answer.get('comments', [])):
            ans_comment_body = clean_html(ans_comment.get('body', ''))
            if ans_comment_body:
                parts.append(f"å›ç­”{j+1}è¯„è®º{k+1}: {ans_comment_body}")
    
    full_text = " ".join(parts)
    
    # æˆªæ–­å¤„ç†
    max_length = 4000
    if len(full_text) > max_length:
        if len(title) > 0:
            title_part = f"æ ‡é¢˜: {title} "
            remaining = max_length - len(title_part)
            full_text = title_part + full_text[len(title_part):remaining]
        else:
            full_text = full_text[:max_length]
    
    return full_text

def load_model_completely_offline():
    """å®Œå…¨ç¦»çº¿åŠ è½½æ¨¡å‹"""
    print("å°è¯•å®Œå…¨ç¦»çº¿åŠ è½½æ¨¡å‹...")
    
    # æŸ¥æ‰¾æœ¬åœ°æ¨¡å‹ç¼“å­˜
    cache_paths = [
        os.path.expanduser('~/.cache/torch/sentence_transformers'),
        os.path.expanduser('~/.cache/huggingface/hub'),
        'D:/.cache/torch/sentence_transformers',
        'D:/.cache/huggingface/hub'
    ]
    
    model_path = None
    for path in cache_paths:
        if os.path.exists(path):
            print(f"æ£€æŸ¥è·¯å¾„: {path}")
            # åœ¨ç¼“å­˜ä¸­æŸ¥æ‰¾æ¨¡å‹
            for root, dirs, files in os.walk(path):
                if 'all-MiniLM-L6-v2' in root and 'modules.json' in files:
                    model_path = root
                    print(f"âœ… æ‰¾åˆ°æ¨¡å‹è·¯å¾„: {model_path}")
                    break
            if model_path:
                break
    
    if model_path:
        try:
            # æ–¹æ³•1ï¼šç›´æ¥ä½¿ç”¨æœ¬åœ°è·¯å¾„
            from sentence_transformers import SentenceTransformer
            model = SentenceTransformer(model_path)
            print("âœ… ä»æœ¬åœ°è·¯å¾„åŠ è½½æˆåŠŸ")
            return model
        except Exception as e:
            print(f"æœ¬åœ°è·¯å¾„åŠ è½½å¤±è´¥: {e}")
    
    # æ–¹æ³•2ï¼šä½¿ç”¨å·²ç»å¯¼å…¥çš„æ¨¡å‹ï¼ˆå¦‚æœä¹‹å‰æˆåŠŸè¿‡ï¼‰
    try:
        print("å°è¯•ä½¿ç”¨å·²ç¼“å­˜çš„æ¨¡å‹...")
        # é‡æ–°å¯¼å…¥ï¼Œåˆ©ç”¨Pythonçš„æ¨¡å—ç¼“å­˜
        import importlib
        import sentence_transformers
        importlib.reload(sentence_transformers)
        from sentence_transformers import SentenceTransformer
        
        # é™é»˜åŠ è½½ï¼Œé¿å…ç½‘ç»œè¯·æ±‚
        import logging
        logging.getLogger("sentence_transformers").setLevel(logging.ERROR)
        logging.getLogger("transformers").setLevel(logging.ERROR)
        
        model = SentenceTransformer('all-MiniLM-L6-v2')
        print("âœ… ä½¿ç”¨ç¼“å­˜æ¨¡å‹æˆåŠŸ")
        return model
    except Exception as e:
        print(f"ç¼“å­˜æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
    
    return None

def main():
    # æ£€æŸ¥ç¯å¢ƒ
    print(f"PyTorchç‰ˆæœ¬: {torch.__version__}")
    print(f"CUDAå¯ç”¨: {torch.cuda.is_available()}")
    
    # 1. è¯»å–åŸå§‹JSONæ•°æ®
    print("\n1. è¯»å–åŸå§‹æ•°æ®...")
    try:
        with open("oracle_database_questions.json", "r", encoding="utf-8") as f:
            data = json.load(f)
        print(f"âœ… æˆåŠŸè¯»å– {len(data):,} æ¡é—®é¢˜")
    except Exception as e:
        print(f"âŒ è¯»å–æ•°æ®å¤±è´¥: {e}")
        return
    
    # 2. æå–å’Œç»„åˆæ‰€æœ‰æ–‡æœ¬å†…å®¹
    print("\n2. ç»„åˆæ–‡æœ¬å†…å®¹...")
    full_texts = []
    question_ids = []
    titles = []
    answer_counts = []
    text_lengths = []
    
    for item in tqdm(data, desc="å¤„ç†é—®é¢˜"):
        question_id = item.get('question_id', '')
        title = clean_html(item.get('title', ''))
        
        full_text = combine_all_content(item)
        
        if full_text and len(full_text) > 10:
            full_texts.append(full_text)
            question_ids.append(question_id)
            titles.append(title)
            answer_counts.append(len(item.get('answers', [])))
            text_lengths.append(len(full_text))
    
    print(f"âœ… æœ‰æ•ˆé—®é¢˜æ•°: {len(full_texts):,}")
    print(f"ğŸ“Š å¹³å‡æ–‡æœ¬é•¿åº¦: {np.mean(text_lengths):.0f} å­—ç¬¦")
    
    # 3. åŠ è½½æ¨¡å‹ï¼ˆå®Œå…¨ç¦»çº¿ï¼‰
    print("\n3. åŠ è½½è¯­ä¹‰æ¨¡å‹ï¼ˆå®Œå…¨ç¦»çº¿ï¼‰...")
    model = load_model_completely_offline()
    
    if model is None:
        print("âŒ æ— æ³•åŠ è½½æ¨¡å‹ï¼Œä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆ...")
        # å¤‡ç”¨æ–¹æ¡ˆï¼šä½¿ç”¨ä¹‹å‰å·²ç»ç”Ÿæˆçš„å‘é‡
        if os.path.exists('oracle_embeddings_cuda12.npy'):
            print("ä½¿ç”¨ä¹‹å‰ç”Ÿæˆçš„æ ‡é¢˜å‘é‡...")
            return
        else:
            print("æ²¡æœ‰å¯ç”¨çš„å‘é‡æ–‡ä»¶")
            return
    
    # 4. å‘é‡åŒ–ç¼–ç 
    print("\n4. å¼€å§‹è¯­ä¹‰å‘é‡åŒ–...")
    start_time = time.time()
    
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    model = model.to(device)
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åˆ†æ‰¹å¤„ç†
    batch_size = 16  # æ›´å°çš„batch_sizeç¡®ä¿ç¨³å®š
    all_embeddings = []
    
    for i in tqdm(range(0, len(full_texts), batch_size), desc="å‘é‡åŒ–æ‰¹æ¬¡"):
        batch_texts = full_texts[i:i+batch_size]
        try:
            # å®Œå…¨é™é»˜å¤„ç†
            with torch.no_grad():
                embeddings = model.encode(
                    batch_texts,
                    batch_size=8,
                    show_progress_bar=False,
                    convert_to_numpy=True,
                    normalize_embeddings=True,
                    device=device
                )
            all_embeddings.append(embeddings)
        except Exception as e:
            print(f"âŒ æ‰¹æ¬¡ {i} å¤„ç†å¤±è´¥: {e}")
            # è·³è¿‡æœ‰é—®é¢˜çš„æ‰¹æ¬¡
            continue
    
    # åˆå¹¶æ‰€æœ‰å‘é‡
    if all_embeddings:
        final_embeddings = np.vstack(all_embeddings)
        print(f"âœ… å‘é‡åŒ–å®Œæˆ! å½¢çŠ¶: {final_embeddings.shape}")
    else:
        print("âŒ å‘é‡åŒ–å¤±è´¥!")
        return
    
    # 5. ä¿å­˜ç»“æœ
    print("\n5. ä¿å­˜ç»“æœ...")
    
    np.save('oracle_full_content_embeddings.npy', final_embeddings)
    
    full_metadata = pd.DataFrame({
        'question_id': question_ids,
        'title': titles,
        'answer_count': answer_counts,
        'text_length': text_lengths
        # ä¸ä¿å­˜full_textï¼Œæ–‡ä»¶å¤ªå¤§
    })
    full_metadata.to_csv('oracle_full_metadata.csv', index=False, encoding='utf-8')
    
    end_time = time.time()
    total_time = end_time - start_time
    
    print(f"\n=== å®Œæˆ! ===")
    print(f"â±ï¸  æ€»è€—æ—¶: {total_time/60:.2f} åˆ†é’Ÿ")
    print(f"ğŸ’¾ è¾“å‡ºæ–‡ä»¶: oracle_full_content_embeddings.npy")

if __name__ == "__main__":
    main()